# Robust Machine Learning and Adversarial Examples

The gap between the recent success of deep learning models and our understanding of them is a
major concern in the machine learning community. In 2013, Szegedy et al. presented a method
of finding a small perturbation which when added to an image causes misclassification by a neural
network classifier (while being barely visible by a human). 

In this repository, I will implement the approaches to find the adversarial noise and its effect on image data (MNIST and CIFR).

## Dependencies

## Methods

## How to Use

## References
